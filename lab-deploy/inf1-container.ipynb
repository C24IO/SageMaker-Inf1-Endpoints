{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# This is the file that implements a flask server to do inferences. It's the file that you will modify to\u001b[39;49;00m\n",
      "\u001b[37m# implement the scoring for your own algorithm.\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m print_function\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msignal\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtraceback\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch_neuron\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BertTokenizer\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BertModel\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmath\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m AutoTokenizer, AutoModelForSequenceClassification\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mio\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36murllib\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mrequest\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34mtry\u001b[39;49;00m:\n",
      "    \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mStringIO\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m StringIO \u001b[37m## for Python 2\u001b[39;49;00m\n",
      "\u001b[34mexcept\u001b[39;49;00m \u001b[36mImportError\u001b[39;49;00m:\n",
      "    \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mio\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m StringIO \u001b[37m## for Python 3\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mflask\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\n",
      "prefix = \u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "model_path = os.path.join(prefix, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "\u001b[37m# A singleton for holding the model. This simply loads the model and holds it.\u001b[39;49;00m\n",
      "\u001b[37m# It has a predict function that does a prediction based on the model and the input data.\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mScoringService\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\n",
      "    model = \u001b[34mNone\u001b[39;49;00m                \u001b[37m# Where we keep the model when it's loaded\u001b[39;49;00m\n",
      "\n",
      "    \u001b[90m@classmethod\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mget_model\u001b[39;49;00m(\u001b[36mcls\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Get the model object for this instance, loading it if it's not already loaded.\"\"\"\u001b[39;49;00m        \n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mcls\u001b[39;49;00m.model == \u001b[34mNone\u001b[39;49;00m:\n",
      "            \u001b[36mcls\u001b[39;49;00m.model = torch.jit.load(os.path.join(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mneuron_compiled_model.pt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mmodel loaded\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[36mcls\u001b[39;49;00m.model)\n",
      "            \n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mcls\u001b[39;49;00m.model\n",
      "\n",
      "    \u001b[90m@classmethod\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mpredict\u001b[39;49;00m(\u001b[36mcls\u001b[39;49;00m, *\u001b[36minput\u001b[39;49;00m):\n",
      "        \n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mpredict\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \u001b[33m\"\"\"For the input, do the predictions and return them.\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m        Args:\u001b[39;49;00m\n",
      "\u001b[33m            input (a pandas dataframe): The data on which to do the predictions. There will be\u001b[39;49;00m\n",
      "\u001b[33m                one prediction per row in the dataframe\"\"\"\u001b[39;49;00m\n",
      "        clf = \u001b[36mcls\u001b[39;49;00m.get_model()\n",
      "        \n",
      "        \u001b[37m#print(type(*input))\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(*\u001b[36minput\u001b[39;49;00m)\n",
      "        \u001b[34mreturn\u001b[39;49;00m clf(*\u001b[36minput\u001b[39;49;00m)\n",
      "\n",
      "\u001b[37m# The flask app for serving predictions\u001b[39;49;00m\n",
      "app = flask.Flask(\u001b[31m__name__\u001b[39;49;00m)\n",
      "\n",
      "\u001b[90m@app\u001b[39;49;00m.route(\u001b[33m'\u001b[39;49;00m\u001b[33m/ping\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, methods=[\u001b[33m'\u001b[39;49;00m\u001b[33mGET\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mping\u001b[39;49;00m():\n",
      "    \u001b[33m\"\"\"Determine if the container is working and healthy. In this sample container, we declare\u001b[39;49;00m\n",
      "\u001b[33m    it healthy if we can load the model successfully.\"\"\"\u001b[39;49;00m\n",
      "    health = ScoringService.get_model() \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m  \u001b[37m# You can insert a health check here\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(health)\n",
      "    \u001b[37m#health = True \u001b[39;49;00m\n",
      "\n",
      "    status = \u001b[34m200\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m health \u001b[34melse\u001b[39;49;00m \u001b[34m404\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m flask.Response(response=\u001b[33m'\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, status=status, mimetype=\u001b[33m'\u001b[39;49;00m\u001b[33mapplication/json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "\u001b[90m@app\u001b[39;49;00m.route(\u001b[33m'\u001b[39;49;00m\u001b[33m/invocations\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, methods=[\u001b[33m'\u001b[39;49;00m\u001b[33mPOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtransformation\u001b[39;49;00m():\n",
      "    \u001b[33m\"\"\"Do an inference on a single batch of data. In this sample server, we take data as CSV, convert\u001b[39;49;00m\n",
      "\u001b[33m    it to a pandas data frame for internal use and then convert the predictions back to CSV (which really\u001b[39;49;00m\n",
      "\u001b[33m    just means one prediction per line, since there's a single column.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    data = \u001b[34mNone\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(flask.request.content_type)\n",
      "        \n",
      "    pickled_bytes = flask.request.data\n",
      "    encoded_sentence_tuple = pickle.loads(pickled_bytes)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[36mtype\u001b[39;49;00m(encoded_sentence_tuple))\n",
      "    \u001b[37m#input_statement = encoded_sentence['input_ids'], encoded_sentence['attention_mask'], encoded_sentence['token_type_ids']  \u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(encoded_sentence_tuple)\n",
      "    embedding = ScoringService.predict(*encoded_sentence_tuple)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[36mtype\u001b[39;49;00m(embedding))\n",
      "    \u001b[36mprint\u001b[39;49;00m(embedding)\n",
      "    raw_bytes_embedding = pickle.dumps(embedding)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mScored\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "         \n",
      "    result = raw_bytes_embedding\n",
      "    \u001b[34mreturn\u001b[39;49;00m flask.Response(response=result, status=\u001b[34m200\u001b[39;49;00m, mimetype=\u001b[33m'\u001b[39;49;00m\u001b[33mapplication/binary\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize code/predictor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Example neuron-rtd dockerfile.\n",
      "\n",
      "# To build:\n",
      "#    docker build . -f Dockerfile.neuron-rtd -t neuron-rtd\n",
      "\n",
      "# Note: the container must start with CAP_SYS_ADMIN + CAP_IPC_LOCK capabilities in order\n",
      "# to map the memory needed from the Infernetia devices. These capabilities will\n",
      "# be dropped following initialization.\n",
      "\n",
      "# i.e. To start the container with required capabilities:\n",
      "#   docker run --env AWS_NEURON_VISIBLE_DEVICES=\"0\" --cap-add SYS_ADMIN --cap-add IPC_LOCK -v /tmp/neuron_rtd_sock/:/sock neuron-rtd\n",
      "\n",
      "FROM amazonlinux:2\n",
      "\n",
      "MAINTAINER Chaitanya Hazarey <chazarey@amazon.com>\n",
      "\n",
      "RUN echo $'[neuron] \\n\\\n",
      "name=Neuron YUM Repository \\n\\\n",
      "baseurl=https://yum.repos.neuron.amazonaws.com \\n\\\n",
      "enabled=1' > /etc/yum.repos.d/neuron.repo\n",
      "\n",
      "RUN rpm --import https://yum.repos.neuron.amazonaws.com/GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB\n",
      "\n",
      "RUN yum install -y wget \n",
      "RUN yum install -y aws-neuron-tools\n",
      "RUN yum install -y aws-neuron-runtime\n",
      "#RUN yum install -y python\n",
      "RUN yum install -y tar gzip ca-certificates procps net-tools which vim\n",
      "RUN amazon-linux-extras install -y nginx1\n",
      "\n",
      "# Here we get all python packages.\n",
      "# There's substantial overlap between scipy and numpy that we eliminate by\n",
      "# linking them together. Likewise, pip leaves the install caches populated which uses\n",
      "# a significant amount of space. These optimizations save a fair amount of space in the\n",
      "# image, which reduces start up time.\n",
      "#/usr/lib64/python2.7/site-packages/scipy\n",
      "\n",
      "\n",
      "RUN yum install -y python3 libgomp\n",
      "RUN wget https://bootstrap.pypa.io/get-pip.py\n",
      "RUN python3 get-pip.py && \\\n",
      "    python3 -m pip install --upgrade --force-reinstall --no-cache-dir 'numpy<=1.18.2,>=1.13.3' \\ \n",
      "    'neuron-cc[tensorflow]>=1.0.16861.0' torch-neuron\\ \n",
      "    scipy==1.2.1 scikit-learn==0.20.2 pandas flask gevent gunicorn \\\n",
      "    transformers==2.5.1 --extra-index-url=https://pip.repos.neuron.amazonaws.com    \n",
      "    \n",
      "# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n",
      "# output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n",
      "# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n",
      "# PATH so that the train and serve programs are found when the container is invoked.\n",
      "\n",
      "RUN cp -r /usr/local/lib/python3.7/site-packages/torch/neuron/ /usr/local/lib64/python3.7/site-packages/torch/\n",
      "\n",
      "ENV NEURONCORE_GROUP_SIZES=1\n",
      "ENV PYTHONUNBUFFERED=TRUE\n",
      "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      "ENV PATH=\"/opt/program:${PATH}\"\n",
      "\n",
      "# Set up the program in the image\n",
      "COPY code /opt/program\n",
      "ENV PATH=\"/opt/aws/neuron/bin:${PATH}\"\n",
      "\n",
      "CMD neuron-rtd -g unix:/sock/neuron.sock --log-console\n",
      "\n",
      "WORKDIR /opt/program\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat Dockerfile.sm.neuron-rtd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code  Dockerfile.sm.neuron-rtd\tinf1-container.ipynb  inference.py  README.md\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh \n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=chazarey-inf1-serving\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "\n",
    "docker build -t ${algorithm_name} -f Dockerfile.sm.neuron-rtd .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install 'sagemaker[local]' --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "role = sagemaker.session.get_execution_role()\n",
    "\n",
    "model_data='s3://inf1-compiled-model/model.tar.gz'\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data=model_data, \n",
    "                             role=role,\n",
    "                             entry_point='inference.py',\n",
    "                             image='111652037296.dkr.ecr.us-east-1.amazonaws.com/chazarey-inf1-serving:latest',\n",
    "                             framework_version='1.5.0',\n",
    "                             enable_cloudwatch_metrics=True)\n",
    "\n",
    "predictor = pytorch_model.deploy(instance_type='ml.inf1.24xlarge', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "import math\n",
    "import numpy as np\n",
    "import io\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pickle \n",
    "\n",
    "def numpy_bytes_serializer(data):\n",
    "    f = io.BytesIO()\n",
    "    np.save(f, data)\n",
    "    f.seek(0)\n",
    "    return f.read()\n",
    "\n",
    "# vnd indicates vendor-specific MIME types, which means they are MIME types that were \n",
    "# introduced by corporate bodies rather than e.g. an Internet consortium.\n",
    "\n",
    "\n",
    "#-predictor.serializer = numpy_bytes_serializer\n",
    "#-predictor.deserializer = csv_serializer\n",
    "\n",
    "predictor.content_type = 'application/binary'\n",
    "predictor.serializer = None\n",
    "predictor.deserializer = None\n",
    "\n",
    "sentence1=\"If you set your goals ridiculously high and it's a failure, you will fail above everyone else's success.\"\n",
    "sentence2=\"The greatest glory in living lies not in never falling, but in rising every time we fall.\"\n",
    "sentence3=\"If you set your goals ridiculously high and it's a failure, you will fail above everyone else's success. If you set your goals ridiculously high and it's a failure, you will fail above everyone else's success. If you set your goals ridiculously high and it's a failure, you will fail above everyone else's success.\"\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "cos = torch.nn.CosineSimilarity()\n",
    "\n",
    "encoded_sentence = tokenizer.encode_plus(sentence1, sentence3, max_length=128, pad_to_max_length=True, return_tensors=\"pt\", truncation=True)\n",
    "encoded_sentence_tuple = encoded_sentence['input_ids'], encoded_sentence['attention_mask'], encoded_sentence['token_type_ids'] \n",
    "\n",
    "try:\n",
    "    pickled_bytes = pickle.dumps(encoded_sentence_tuple)\n",
    "    raw_bytes = predictor.predict(pickled_bytes)\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "    \n",
    "#print(pickle.loads(raw_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(encoded_sentence_tuple)#\n",
    "#import pickle \n",
    "#msg = pickle.dumps(encoded_sentence_tuple)\n",
    "#recd = pickle.loads(msg)\n",
    "#type(recd)\n",
    "#recd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "s_nouns = [\"A dude\", \"My mom\", \"The king\", \"Some guy\", \"A cat with rabies\", \"A sloth\", \"Your homie\", \"This cool guy my gardener met yesterday\", \"Superman\"]\n",
    "p_nouns = [\"These dudes\", \"Both of my moms\", \"All the kings of the world\", \"Some guys\", \"All of a cattery's cats\", \"The multitude of sloths living under your bed\", \"Your homies\", \"Like, these, like, all these people\", \"Supermen\"]\n",
    "s_verbs = [\"eats\", \"kicks\", \"gives\", \"treats\", \"meets with\", \"creates\", \"hacks\", \"configures\", \"spies on\", \"retards\", \"meows on\", \"flees from\", \"tries to automate\", \"explodes\"]\n",
    "p_verbs = [\"eat\", \"kick\", \"give\", \"treat\", \"meet with\", \"create\", \"hack\", \"configure\", \"spy on\", \"retard\", \"meow on\", \"flee from\", \"try to automate\", \"explode\"]\n",
    "infinitives = [\"to make a pie.\", \"for no apparent reason.\", \"because the sky is green.\", \"for a disease.\", \"to be able to make toast explode.\", \"to know more about archeology.\"]\n",
    "\n",
    "for i in range(0,10):    \n",
    "    sentence_random = (random.choice(s_nouns) + ' ' + random.choice(s_verbs) + ' ' + random.choice(s_nouns).lower() or random.choice(p_nouns).lower() + ' ' + random.choice(infinitives))\n",
    "    encoded_sentence = tokenizer.encode_plus(sentence1, sentence_random, max_length=128, pad_to_max_length=True, return_tensors=\"pt\", truncation=True)\n",
    "    encoded_sentence_tuple = encoded_sentence['input_ids'], encoded_sentence['attention_mask'], encoded_sentence['token_type_ids'] \n",
    "    pickled_bytes = pickle.dumps(encoded_sentence_tuple)\n",
    "    \n",
    "    try:\n",
    "        raw_bytes = predictor.predict(pickled_bytes)\n",
    "        embeddings_returned = pickle.loads(raw_bytes)\n",
    "        s1 = embeddings_returned[1]\n",
    "        #print(pickle.loads(raw_bytes))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    sentence_random = (random.choice(s_nouns) + ' ' + random.choice(s_verbs) + ' ' + random.choice(s_nouns).lower() or random.choice(p_nouns).lower() + ' ' + random.choice(infinitives))\n",
    "    encoded_sentence = tokenizer.encode_plus(sentence1, sentence_random, max_length=128, pad_to_max_length=True, return_tensors=\"pt\", truncation=True)\n",
    "    encoded_sentence_tuple = encoded_sentence['input_ids'], encoded_sentence['attention_mask'], encoded_sentence['token_type_ids'] \n",
    "    pickled_bytes = pickle.dumps(encoded_sentence_tuple)\n",
    "    \n",
    "    try:\n",
    "        raw_bytes = predictor.predict(pickled_bytes)\n",
    "        embeddings_returned = pickle.loads(raw_bytes)\n",
    "        s2 = embeddings_returned[1]\n",
    "        #print(pickle.loads(raw_bytes))\n",
    "    except:\n",
    "        pass    \n",
    "    \n",
    "    cos = torch.nn.CosineSimilarity()    \n",
    "    s2 = embeddings_returned[1]\n",
    "    cos_sim = cos(s1,s2)\n",
    "    cosine_measure = cos_sim[0].item()\n",
    "    angle_in_radians = math.acos(cosine_measure)\n",
    "    print(math.degrees(angle_in_radians))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import datetime\n",
    "import math\n",
    "import time\n",
    "import boto3   \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "endpoint_name=predictor.endpoint\n",
    "total_runs=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Running {} inferences for {}:'.format(total_runs, endpoint_name))\n",
    "\n",
    "client_times = []\n",
    "errors_list = []\n",
    "cw_start = datetime.datetime.utcnow()\n",
    "\n",
    "errors = 0\n",
    "\n",
    "for i in range(total_runs):    \n",
    "    \n",
    "    client_start = time.time()\n",
    "    \n",
    "    sentence_random = (random.choice(s_nouns) + ' ' + random.choice(s_verbs) + ' ' + random.choice(s_nouns).lower() or random.choice(p_nouns).lower() + ' ' + random.choice(infinitives))\n",
    "    encoded_sentence = tokenizer.encode_plus(sentence1, sentence_random, max_length=128, pad_to_max_length=True, return_tensors=\"pt\", truncation=True)\n",
    "    encoded_sentence_tuple = encoded_sentence['input_ids'], encoded_sentence['attention_mask'], encoded_sentence['token_type_ids'] \n",
    "    \n",
    "    pickled_bytes = pickle.dumps(encoded_sentence_tuple)\n",
    "\n",
    "    try:\n",
    "        raw_bytes = predictor.predict(pickled_bytes)\n",
    "        errors_list.append(20)\n",
    "    except:\n",
    "        errors += 1\n",
    "        errors_list.append(30)\n",
    "        pass\n",
    "    \n",
    "    client_end = time.time()\n",
    "    client_times.append((client_end - client_start)*1000)\n",
    "    \n",
    "print('\\nErrors - {:.4f} out of {:.4f} total runs | {:.4f}% \\n'.format(errors, total_runs, (errors/total_runs)*100))\n",
    "errors = 0\n",
    "    \n",
    "    \n",
    "cw_end = datetime.datetime.utcnow()    \n",
    "    \n",
    "print('Client end-to-end latency percentiles:')\n",
    "client_avg = np.mean(client_times)\n",
    "client_p50 = np.percentile(client_times, 50)\n",
    "client_p90 = np.percentile(client_times, 90)\n",
    "client_p95 = np.percentile(client_times, 95)\n",
    "client_p100 = np.percentile(client_times, 100)\n",
    "print('Avg | P50 | P90 | P95 | P100')\n",
    "print('{:.4f} | {:.4f} | {:.4f} | {:.4f}\\n'.format(client_avg, client_p50, client_p90, client_p95, client_p100))\n",
    "\n",
    "print('Getting Cloudwatch:')\n",
    "cloudwatch = boto3.client('cloudwatch')\n",
    "statistics=['SampleCount', 'Average', 'Minimum', 'Maximum']\n",
    "extended=['p50', 'p90', 'p95', 'p100']\n",
    "\n",
    "# Give 5 minute buffer to end\n",
    "cw_end += datetime.timedelta(minutes=5)\n",
    "\n",
    "# Period must be 1, 5, 10, 30, or multiple of 60\n",
    "# Calculate closest multiple of 60 to the total elapsed time\n",
    "factor = math.ceil((cw_end - cw_start).total_seconds() / 60)\n",
    "period = factor * 60\n",
    "print('Time elapsed: {} seconds'.format((cw_end - cw_start).total_seconds()))\n",
    "print('Using period of {} seconds\\n'.format(period))\n",
    "\n",
    "cloudwatch_ready = False\n",
    "# Keep polling CloudWatch metrics until datapoints are available\n",
    "while not cloudwatch_ready:\n",
    "  time.sleep(30)\n",
    "  print('Waiting 30 seconds ...')\n",
    "  # Must use default units of microseconds\n",
    "  model_latency_metrics = cloudwatch.get_metric_statistics(MetricName='ModelLatency',\n",
    "                                             Dimensions=[{'Name': 'EndpointName',\n",
    "                                                          'Value': endpoint_name},\n",
    "                                                         {'Name': 'VariantName',\n",
    "                                                          'Value': \"AllTraffic\"}],\n",
    "                                             Namespace=\"AWS/SageMaker\",\n",
    "                                             StartTime=cw_start,\n",
    "                                             EndTime=cw_end,\n",
    "                                             Period=period,\n",
    "                                             Statistics=statistics,\n",
    "                                             ExtendedStatistics=extended\n",
    "                                             )\n",
    "  # Should be 1000\n",
    "  if len(model_latency_metrics['Datapoints']) > 0:\n",
    "    print('{} latency datapoints ready'.format(model_latency_metrics['Datapoints'][0]['SampleCount']))\n",
    "    side_avg = model_latency_metrics['Datapoints'][0]['Average'] / total_runs\n",
    "    side_p50 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p50'] / total_runs\n",
    "    side_p90 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p90'] / total_runs\n",
    "    side_p95 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p95'] / total_runs\n",
    "    side_p100 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p100'] / total_runs\n",
    "    print('Avg | P50 | P90 | P95 | P100')\n",
    "    print('{:.4f} | {:.4f} | {:.4f} | {:.4f}\\n'.format(side_avg, side_p50, side_p90, side_p95, side_p100))\n",
    "\n",
    "    cloudwatch_ready = True\n",
    "    \n",
    "    #embeddings_returned = pickle.loads(raw_bytes)\n",
    "    #s1 = embeddings_returned[1]\n",
    "    #print(pickle.loads(raw_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(80, 60))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(client_times)\n",
    "ax.plot(errors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = '''client_times = []\n",
    "cw_start = datetime.datetime.utcnow()\n",
    "\n",
    "client_start = time.time()\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "client_end = time.time()\n",
    "\n",
    "client_times.append((client_end - client_start))\n",
    "\n",
    "cw_end = datetime.datetime.utcnow()    \n",
    "    \n",
    "print('Client end-to-end latency percentiles:')\n",
    "client_avg = np.mean(client_times)\n",
    "client_p50 = np.percentile(client_times, 50)\n",
    "client_p90 = np.percentile(client_times, 90)\n",
    "client_p95 = np.percentile(client_times, 95)\n",
    "client_p100 = np.percentile(client_times, 100)\n",
    "print('Avg | P50 | P90 | P95 | P100')\n",
    "print('{:.4f} | {:.4f} | {:.4f} | {:.4f}\\n'.format(client_avg, client_p50, client_p90, client_p95, client_p100))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
