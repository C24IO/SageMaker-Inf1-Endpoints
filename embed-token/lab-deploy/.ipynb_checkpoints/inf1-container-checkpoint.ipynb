{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pygmentize code/predictor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "#model = BertModel.from_pretrained('bert-base-uncased')\n",
    "#tokenizer.save_pretrained('./bert-base-uncased-saved/')\n",
    "#model.save_pretrained('./bert-base-uncased-saved/')\n",
    "#!(cd bert-base-uncased-saved ; tar -czvf bert-base-uncased-saved.tar.gz *)\n",
    "#!(cd bert-base-uncased-saved ; aws s3 cp bert-base-uncased-saved.tar.gz s3://inf1-compiled-model/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!aws s3 ls s3://inf1-compiled-model/bert-base-uncased-saved.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!aws s3 cp s3://inf1-compiled-model/bert-base-uncased-saved.tar.gz . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir bert-base-uncased-saved\n",
    "#!(cd bert-base-uncased-saved ; tar -xzvf ../bert-base-uncased-saved.tar.gz) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cat Dockerfile.sm.neuron-rtd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased-saved\t\tDockerfile.sm.neuron-rtd  README.md\n",
      "bert-base-uncased-saved.tar.gz\tinf1-container.ipynb\n",
      "code\t\t\t\tinference.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Sending build context to Docker daemon  843.6MB\n",
      "Step 1/22 : FROM amazonlinux:2\n",
      " ---> ba2cc467a2bc\n",
      "Step 2/22 : MAINTAINER Chaitanya Hazarey <chazarey@amazon.com>\n",
      " ---> Using cache\n",
      " ---> 088a31d5941a\n",
      "Step 3/22 : RUN echo $'[neuron] \\nname=Neuron YUM Repository \\nbaseurl=https://yum.repos.neuron.amazonaws.com \\nenabled=1' > /etc/yum.repos.d/neuron.repo\n",
      " ---> Using cache\n",
      " ---> 0769d0beed1b\n",
      "Step 4/22 : RUN rpm --import https://yum.repos.neuron.amazonaws.com/GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB\n",
      " ---> Using cache\n",
      " ---> 5eae2d75e8f3\n",
      "Step 5/22 : RUN yum install -y wget\n",
      " ---> Using cache\n",
      " ---> dfd9fed5d8f0\n",
      "Step 6/22 : RUN yum install -y aws-neuron-tools\n",
      " ---> Using cache\n",
      " ---> 276a5f678a9e\n",
      "Step 7/22 : RUN yum install -y aws-neuron-runtime\n",
      " ---> Using cache\n",
      " ---> 1fd9585a3950\n",
      "Step 8/22 : RUN yum install -y tar gzip ca-certificates procps net-tools which vim\n",
      " ---> Using cache\n",
      " ---> 43815f81991a\n",
      "Step 9/22 : RUN amazon-linux-extras install -y nginx1\n",
      " ---> Using cache\n",
      " ---> d3b2f457d5e9\n",
      "Step 10/22 : RUN yum install -y python3 libgomp\n",
      " ---> Using cache\n",
      " ---> f0560c313050\n",
      "Step 11/22 : RUN wget https://bootstrap.pypa.io/get-pip.py\n",
      " ---> Using cache\n",
      " ---> 56c0a525e4dc\n",
      "Step 12/22 : RUN python3 get-pip.py &&     python3 -m pip install --upgrade --force-reinstall --no-cache-dir 'numpy<=1.18.2,>=1.13.3'     'neuron-cc[tensorflow]>=1.0.16861.0' torch-neuron    scipy==1.2.1 scikit-learn==0.20.2 pandas flask gevent gunicorn     transformers==2.5.1 --extra-index-url=https://pip.repos.neuron.amazonaws.com\n",
      " ---> Using cache\n",
      " ---> f4f78c6ff232\n",
      "Step 13/22 : RUN cp -r /usr/local/lib/python3.7/site-packages/torch/neuron/ /usr/local/lib64/python3.7/site-packages/torch/\n",
      " ---> Using cache\n",
      " ---> a144e5df0017\n",
      "Step 14/22 : ENV NEURONCORE_GROUP_SIZES=1\n",
      " ---> Using cache\n",
      " ---> 9b6c98e03c52\n",
      "Step 15/22 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> 159439af945b\n",
      "Step 16/22 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Using cache\n",
      " ---> 827cafb7f855\n",
      "Step 17/22 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 1d91d77403ca\n",
      "Step 18/22 : COPY bert-base-uncased-saved /opt/program/bert-base-uncased-saved\n",
      " ---> Using cache\n",
      " ---> 3a80ad123368\n",
      "Step 19/22 : COPY code /opt/program\n",
      " ---> Using cache\n",
      " ---> 00804bc10dfa\n",
      "Step 20/22 : ENV PATH=\"/opt/aws/neuron/bin:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> fa1384056531\n",
      "Step 21/22 : CMD neuron-rtd -g unix:/sock/neuron.sock --log-console\n",
      " ---> Using cache\n",
      " ---> 1b0d75d788bc\n",
      "Step 22/22 : WORKDIR /opt/program\n",
      " ---> Using cache\n",
      " ---> 4cc12a8f4685\n",
      "Successfully built 4cc12a8f4685\n",
      "Successfully tagged chazarey-inf1-serving:latest\n",
      "The push refers to repository [111652037296.dkr.ecr.us-east-1.amazonaws.com/chazarey-inf1-serving]\n",
      "108685e1506c: Preparing\n",
      "d733c91fdba3: Preparing\n",
      "abccac9a4d69: Preparing\n",
      "2e4edcabd260: Preparing\n",
      "36db74e707f0: Preparing\n",
      "faa787afd20e: Preparing\n",
      "632481ba49f2: Preparing\n",
      "af86c485bd82: Preparing\n",
      "871961f580a8: Preparing\n",
      "acbd810c8f6d: Preparing\n",
      "c3ad1ea4248c: Preparing\n",
      "20677aae3d40: Preparing\n",
      "3370ed55d11c: Preparing\n",
      "50c3cd231426: Preparing\n",
      "acbd810c8f6d: Waiting\n",
      "c3ad1ea4248c: Waiting\n",
      "20677aae3d40: Waiting\n",
      "3370ed55d11c: Waiting\n",
      "50c3cd231426: Waiting\n",
      "faa787afd20e: Waiting\n",
      "632481ba49f2: Waiting\n",
      "af86c485bd82: Waiting\n",
      "871961f580a8: Waiting\n",
      "108685e1506c: Layer already exists\n",
      "d733c91fdba3: Layer already exists\n",
      "2e4edcabd260: Layer already exists\n",
      "abccac9a4d69: Layer already exists\n",
      "36db74e707f0: Layer already exists\n",
      "faa787afd20e: Layer already exists\n",
      "632481ba49f2: Layer already exists\n",
      "af86c485bd82: Layer already exists\n",
      "acbd810c8f6d: Layer already exists\n",
      "871961f580a8: Layer already exists\n",
      "c3ad1ea4248c: Layer already exists\n",
      "3370ed55d11c: Layer already exists\n",
      "20677aae3d40: Layer already exists\n",
      "50c3cd231426: Layer already exists\n",
      "latest: digest: sha256:22486efe12b2f6d24a98d12fff0c4fc67df92e202e2c7b204c55b4020bb5d585 size: 3277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh \n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=chazarey-inf1-serving\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "\n",
    "docker build -t ${algorithm_name} -f Dockerfile.sm.neuron-rtd .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install 'sagemaker[local]' --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "Your model is not compiled. Please compile your model before using Inferentia.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!CPU times: user 18.7 s, sys: 2.75 s, total: 21.4 s\n",
      "Wall time: 8min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "role = sagemaker.session.get_execution_role()\n",
    "\n",
    "model_data='s3://inf1-compiled-model/model.tar.gz'\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data=model_data, \n",
    "                             role=role,\n",
    "                             entry_point='inference.py',\n",
    "                             image='111652037296.dkr.ecr.us-east-1.amazonaws.com/chazarey-inf1-serving:latest',\n",
    "                             framework_version='1.5.0',\n",
    "                             enable_cloudwatch_metrics=True)\n",
    "\n",
    "predictor = pytorch_model.deploy(instance_type='ml.inf1.xlarge', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chazarey-inf1-serving-2020-09-14-16-14-23-416\n"
     ]
    }
   ],
   "source": [
    "print(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (3.1.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.8.1rc2)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2020.6.8)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (4.47.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging->transformers) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A dude retards superman\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "import math\n",
    "import numpy as np\n",
    "import io\n",
    "from sagemaker.predictor import json_serializer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pickle \n",
    "import json\n",
    "\n",
    "def random_sentence():\n",
    "    \n",
    "    s_nouns = [\"A dude\", \"My mom\", \"The king\", \"Some guy\", \"A cat with rabies\", \"A sloth\", \"Your homie\", \"This cool guy my gardener met yesterday\", \"Superman\"]\n",
    "    p_nouns = [\"These dudes\", \"Both of my moms\", \"All the kings of the world\", \"Some guys\", \"All of a cattery's cats\", \"The multitude of sloths living under your bed\", \"Your homies\", \"Like, these, like, all these people\", \"Supermen\"]\n",
    "    s_verbs = [\"eats\", \"kicks\", \"gives\", \"treats\", \"meets with\", \"creates\", \"hacks\", \"configures\", \"spies on\", \"retards\", \"meows on\", \"flees from\", \"tries to automate\", \"explodes\"]\n",
    "    p_verbs = [\"eat\", \"kick\", \"give\", \"treat\", \"meet with\", \"create\", \"hack\", \"configure\", \"spy on\", \"retard\", \"meow on\", \"flee from\", \"try to automate\", \"explode\"]\n",
    "    infinitives = [\"to make a pie.\", \"for no apparent reason.\", \"because the sky is green.\", \"for a disease.\", \"to be able to make toast explode.\", \"to know more about archeology.\"]\n",
    "    \n",
    "    return (random.choice(s_nouns) + ' ' + random.choice(s_verbs) + ' ' + random.choice(s_nouns).lower() or random.choice(p_nouns).lower() + ' ' + random.choice(infinitives))\n",
    "\n",
    "print(random_sentence())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.accept = 'application/json'\n",
    "predictor.content_type = 'application/json'\n",
    "predictor.serializer = json_serializer\n",
    "predictor.deserializer = None\n",
    "\n",
    "\n",
    "input_data = {}\n",
    "input_data['sentence1'] = random_sentence()\n",
    "input_data['sentence2'] = random_sentence()\n",
    "json_data = json.dumps(input_data)\n",
    "embedding_returned = predictor.predict(json_data)\n",
    "embedding_returned = pickle.loads(embedding_returned)\n",
    "s1 = embedding_returned[1]\n",
    "#print(pickle.loads(raw_bytes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.949707011162513\n",
      "4.988334253443685\n",
      "1.9931161171446772\n",
      "13.931057721021311\n",
      "15.776429377265936\n",
      "47.09833590631524\n",
      "15.868477279137611\n",
      "31.990970602412656\n",
      "44.92401845753722\n",
      "7.15963217100164\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import json_serializer, json_deserializer\n",
    "import torch\n",
    "\n",
    "predictor.accept = 'application/json'\n",
    "predictor.content_type = 'application/json'\n",
    "predictor.serializer = json_serializer\n",
    "predictor.deserializer = None\n",
    "\n",
    "for i in range(0,10):    \n",
    "       \n",
    "    try:\n",
    "        input_data = {}\n",
    "        input_data['sentence1'] = random_sentence()\n",
    "        input_data['sentence2'] = random_sentence()\n",
    "        json_data = json.dumps(input_data)\n",
    "        embedding_returned = predictor.predict(json_data)\n",
    "        embedding_returned = pickle.loads(embedding_returned)\n",
    "        s1 = embedding_returned[1]\n",
    "        #print(pickle.loads(raw_bytes))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        input_data = {}\n",
    "        input_data['sentence1'] = random_sentence()\n",
    "        input_data['sentence2'] = random_sentence()\n",
    "        json_data = json.dumps(input_data)\n",
    "        embedding_returned = predictor.predict(json_data)\n",
    "        embedding_returned = pickle.loads(embedding_returned)\n",
    "        s2 = embedding_returned[1]\n",
    "        #print(pickle.loads(raw_bytes))\n",
    "    except:\n",
    "        pass    \n",
    "    \n",
    "    cos = torch.nn.CosineSimilarity()    \n",
    "    cos_sim = cos(s1,s2)\n",
    "    cosine_measure = cos_sim[0].item()\n",
    "    angle_in_radians = math.acos(cosine_measure)\n",
    "    print(math.degrees(angle_in_radians))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import datetime\n",
    "import math\n",
    "import time\n",
    "import boto3   \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "endpoint_name=predictor.endpoint\n",
    "total_runs=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1000 inferences for chazarey-inf1-serving-2020-09-14-16-14-23-416:\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import json_serializer, json_deserializer\n",
    "\n",
    "predictor.accept = 'application/json'\n",
    "predictor.content_type = 'application/json'\n",
    "predictor.serializer = json_serializer\n",
    "predictor.deserializer = None\n",
    "\n",
    "print('Running {} inferences for {}:'.format(total_runs, endpoint_name))\n",
    "\n",
    "client_times = []\n",
    "errors_list = []\n",
    "cw_start = datetime.datetime.utcnow()\n",
    "\n",
    "errors = 0\n",
    "\n",
    "for i in range(total_runs):    \n",
    "    \n",
    "    client_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        input_data = {}\n",
    "        input_data['sentence1'] = random_sentence()\n",
    "        input_data['sentence2'] = random_sentence()\n",
    "        json_data = json.dumps(input_data)\n",
    "        embedding_returned = predictor.predict(json_data)\n",
    "        embedding_returned = pickle.loads(embedding_returned)\n",
    "        s1 = embedding_returned[1]\n",
    "        errors_list.append(20)\n",
    "    except:\n",
    "        errors += 1\n",
    "        errors_list.append(30)\n",
    "        pass\n",
    "    \n",
    "    client_end = time.time()\n",
    "    client_times.append((client_end - client_start)*1000)\n",
    "    \n",
    "print('\\nErrors - {:.4f} out of {:.4f} total runs | {:.4f}% \\n'.format(errors, total_runs, (errors/total_runs)*100))\n",
    "errors = 0\n",
    "    \n",
    "    \n",
    "cw_end = datetime.datetime.utcnow()    \n",
    "    \n",
    "print('Client end-to-end latency percentiles:')\n",
    "client_avg = np.mean(client_times)\n",
    "client_p50 = np.percentile(client_times, 50)\n",
    "client_p90 = np.percentile(client_times, 90)\n",
    "client_p95 = np.percentile(client_times, 95)\n",
    "client_p100 = np.percentile(client_times, 100)\n",
    "print('Avg | P50 | P90 | P95 | P100')\n",
    "print('{:.4f} | {:.4f} | {:.4f} | {:.4f}\\n'.format(client_avg, client_p50, client_p90, client_p95, client_p100))\n",
    "\n",
    "print('Getting Cloudwatch:')\n",
    "cloudwatch = boto3.client('cloudwatch')\n",
    "statistics=['SampleCount', 'Average', 'Minimum', 'Maximum']\n",
    "extended=['p50', 'p90', 'p95', 'p100']\n",
    "\n",
    "# Give 5 minute buffer to end\n",
    "cw_end += datetime.timedelta(minutes=5)\n",
    "\n",
    "# Period must be 1, 5, 10, 30, or multiple of 60\n",
    "# Calculate closest multiple of 60 to the total elapsed time\n",
    "factor = math.ceil((cw_end - cw_start).total_seconds() / 60)\n",
    "period = factor * 60\n",
    "print('Time elapsed: {} seconds'.format((cw_end - cw_start).total_seconds()))\n",
    "print('Using period of {} seconds\\n'.format(period))\n",
    "\n",
    "cloudwatch_ready = False\n",
    "# Keep polling CloudWatch metrics until datapoints are available\n",
    "while not cloudwatch_ready:\n",
    "  time.sleep(30)\n",
    "  print('Waiting 30 seconds ...')\n",
    "  # Must use default units of microseconds\n",
    "  model_latency_metrics = cloudwatch.get_metric_statistics(MetricName='ModelLatency',\n",
    "                                             Dimensions=[{'Name': 'EndpointName',\n",
    "                                                          'Value': endpoint_name},\n",
    "                                                         {'Name': 'VariantName',\n",
    "                                                          'Value': \"AllTraffic\"}],\n",
    "                                             Namespace=\"AWS/SageMaker\",\n",
    "                                             StartTime=cw_start,\n",
    "                                             EndTime=cw_end,\n",
    "                                             Period=period,\n",
    "                                             Statistics=statistics,\n",
    "                                             ExtendedStatistics=extended\n",
    "                                             )\n",
    "  # Should be 1000\n",
    "  if len(model_latency_metrics['Datapoints']) > 0:\n",
    "    print('{} latency datapoints ready'.format(model_latency_metrics['Datapoints'][0]['SampleCount']))\n",
    "    side_avg = model_latency_metrics['Datapoints'][0]['Average'] / total_runs\n",
    "    side_p50 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p50'] / total_runs\n",
    "    side_p90 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p90'] / total_runs\n",
    "    side_p95 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p95'] / total_runs\n",
    "    side_p100 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p100'] / total_runs\n",
    "    print('Avg | P50 | P90 | P95 | P100')\n",
    "    print('{:.4f} | {:.4f} | {:.4f} | {:.4f}\\n'.format(side_avg, side_p50, side_p90, side_p95, side_p100))\n",
    "\n",
    "    cloudwatch_ready = True\n",
    "    \n",
    "    #embeddings_returned = pickle.loads(raw_bytes)\n",
    "    #s1 = embeddings_returned[1]\n",
    "    #print(pickle.loads(raw_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(80, 60))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(client_times)\n",
    "ax.plot(errors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = '''client_times = []\n",
    "cw_start = datetime.datetime.utcnow()\n",
    "\n",
    "client_start = time.time()\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "client_end = time.time()\n",
    "\n",
    "client_times.append((client_end - client_start))\n",
    "\n",
    "cw_end = datetime.datetime.utcnow()    \n",
    "    \n",
    "print('Client end-to-end latency percentiles:')\n",
    "client_avg = np.mean(client_times)\n",
    "client_p50 = np.percentile(client_times, 50)\n",
    "client_p90 = np.percentile(client_times, 90)\n",
    "client_p95 = np.percentile(client_times, 95)\n",
    "client_p100 = np.percentile(client_times, 100)\n",
    "print('Avg | P50 | P90 | P95 | P100')\n",
    "print('{:.4f} | {:.4f} | {:.4f} | {:.4f}\\n'.format(client_avg, client_p50, client_p90, client_p95, client_p100))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
